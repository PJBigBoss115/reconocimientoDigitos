{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"},"colab":{"name":"2. Reconocimiento de dígitos.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"K9FHxrmxnDel"},"source":["<h1><font color=\"#113D68\" size=6>Deep Learning con Python y Keras</font></h1>\n","\n","<h1><font color=\"#113D68\" size=5>Parte 5. Redes Neuronales Convolucionales</font></h1>\n","\n","<h1><font color=\"#113D68\" size=4>2. Reconocimiento de dígitos</font></h1>\n","\n","<br><br>\n","<div style=\"text-align: right\">\n","<font color=\"#113D68\" size=3>Manuel Castillo Cara</font><br>\n","\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"yyVOhqK8nDev"},"source":["---\n","\n","<a id=\"indice\"></a>\n","<h2><font color=\"#004D7F\" size=5>Índice</font></h2>\n","\n","* [0. Contexto](#section0)\n","* [1. MNIST dataset](#section1)\n","* [2. Cargar MNIST](#section2)\n","* [3. Modelo de línea de base con MLP](#section3)\n","* [4. CNN para MNIST](#section4)\n","* [5. CNN más profunda para MNIST](#section5)"]},{"cell_type":"markdown","metadata":{"id":"7xUEES82nDew"},"source":["---\n","<a id=\"section0\"></a>\n","# <font color=\"#004D7F\" size=6> 0. Contexto</font>"]},{"cell_type":"markdown","metadata":{"id":"QwjeagCWnDex"},"source":["En este proyecto, descubrirá cómo desarrollar un modelo de Deep Learning en la tarea de reconocimiento de dígitos manuscritos del MNIST. Después de completar esta clase sabrá:\n","* Cómo cargar MNIST y desarrollar un modelo de red neuronal.\n","* Cómo implementar y evaluar una CNN de línea base para MNIST.\n","* Cómo implementar un modelo de Deep Learning avanzado para MNIST."]},{"cell_type":"code","metadata":{"id":"QXTSz97onDex","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620681575931,"user_tz":360,"elapsed":154189,"user":{"displayName":"HENRRY WALDEMAR SONTAY CHAN","photoUrl":"","userId":"13180258940314725711"}},"outputId":"6b8d83e6-a266-46d5-cf64-98987bb4b8d6"},"source":["import tensorflow as tf\n","# Eliminar warning\n","tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GJztWiOjnDex"},"source":["---\n","<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"Mj1BtYifnDey"},"source":["<a id=\"section1\"></a>\n","# <font color=\"#004D7F\" size=6>1. MNIST dataset</font>"]},{"cell_type":"markdown","metadata":{"id":"CmgCoRlMnDey"},"source":["MNIST toma imágenes de dígitos de una variedad de documentos escaneados, normalizados en tamaño y centrados.\n","\n","Cada imagen es está dada en blanco y negro con $28 × 28$ píxeles (784 píxeles en total). Se usan 60,000 imágenes para entrenar un modelo y 10,000 imágenes para validarlo.\n","\n","Es una tarea de reconocimiento de dígitos. Como tal, hay 10 dígitos (0 a 9) o 10 clases para predecir. \n","\n","En la página web de Rodrigo Benenson hay una lista de los resultados más avanzados y enlaces a los artículos relevantes sobre el MNIST y otros conjuntos de datos."]},{"cell_type":"markdown","metadata":{"id":"Jk7IGobWnDey"},"source":["<div class=\"alert alert-block alert-info\">\n","    \n","<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i>\n","Más información sobre el dataset [MNIST](http://yann.lecun.com/exdb/mnist/)"]},{"cell_type":"markdown","metadata":{"id":"WOOJKuKtnDez"},"source":["<div class=\"alert alert-block alert-info\">\n","    \n","<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i>\n","Información de los resultados sobre MNIST de [Rodrigo Benenson](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html)"]},{"cell_type":"markdown","metadata":{"id":"dBRF--h3nDez"},"source":["---\n","<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"25YqTpeVnDez"},"source":["<a id=\"section2\"></a>\n","# <font color=\"#004D7F\" size=6>2. Cargar MNIST</font>"]},{"cell_type":"markdown","metadata":{"id":"v5kaqoWtnDez"},"source":["El conjunto de datos se descarga automáticamente la primera vez que se llama a esta función y se almacena en su directorio de inicio en `~/.keras/datasets/mnist.pkl.gz` como un archivo de 15 megabytes. \n","\n","Primero escribiremos un pequeño script para descargar y visualizar las primeras 4 imágenes mediante la función `mnist.load data()`."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":305},"id":"ZM4vwwl1nDe0","executionInfo":{"status":"ok","timestamp":1620488179920,"user_tz":360,"elapsed":1304,"user":{"displayName":"HENRRY WALDEMAR SONTAY CHAN","photoUrl":"","userId":"13180258940314725711"}},"outputId":"9ac3d5d1-699c-4648-c2c7-9d5fb77b581b"},"source":["# Plot ad hoc mnist instances\n","from keras.datasets import mnist\n","import matplotlib.pyplot as plt\n","\n","# load (downloaded if needed) the MNIST dataset\n","(X_train, y_train), (X_test, y_test) = mnist.load_data()\n","\n","# plot 4 images as gray scale\n","plt.subplot(221)\n","plt.imshow(X_train[0], cmap=plt.get_cmap('gray'))\n","plt.subplot(222)\n","plt.imshow(X_train[1], cmap=plt.get_cmap('gray'))\n","plt.subplot(223)\n","plt.imshow(X_train[2], cmap=plt.get_cmap('gray'))\n","plt.subplot(224)\n","plt.imshow(X_train[3], cmap=plt.get_cmap('gray'))\n","\n","# show the plot\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11493376/11490434 [==============================] - 0s 0us/step\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 4 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"1TBa88B6nDe1"},"source":["---\n","<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"Ie0joAaKnDe1"},"source":["<a id=\"section3\"></a>\n","# <font color=\"#004D7F\" size=6>3. MLP de línea base</font>"]},{"cell_type":"markdown","metadata":{"id":"kKcntOOonDe1"},"source":["Vamos a usar un MLP clásico como base para la comparación con modelos de redes neuronales convolucionales. \n","\n","Importamos las clases, funciones y el dataset MNIST."]},{"cell_type":"code","metadata":{"id":"6K39Eqp7nDe2"},"source":["# Baseline MLP for MNIST dataset\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.utils import np_utils\n","\n","# load data\n","(X_train, y_train), (X_test, y_test) = mnist.load_data()\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tzpxYVxKnDe2"},"source":["Para un MLP clásico debemos reducir las imágenes a un vector de píxeles. En este caso, las imágenes de tamaño $28 × 28$ serán vectores de entrada de 784 píxeles. \n","\n","Realizamos esta transformación meidante la función `reshape()`. \n","\n","Los valores de los píxeles son números enteros, por lo que los convertimos a punto flotante para poder normalizarlos."]},{"cell_type":"code","metadata":{"id":"tO3bgVETnDe2"},"source":["# flatten 28*28 images to a 784 vector for each image\n","num_pixeles = X_train.shape[1] * X_train.shape[2]\n","\n","X_train = X_train.reshape((X_train.shape[0], num_pixeles)).astype('float32')\n","X_test = X_test.reshape((X_test.shape[0], num_pixeles)).astype('float32')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X2VsW7rToulq","executionInfo":{"status":"ok","timestamp":1620488558656,"user_tz":360,"elapsed":339,"user":{"displayName":"HENRRY WALDEMAR SONTAY CHAN","photoUrl":"","userId":"13180258940314725711"}},"outputId":"a3dd3ac8-69d8-4ff6-ad8e-ca79ad58e758"},"source":["X_train.shape[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["60000"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"2Dt5Jyh5nDe2"},"source":["Los valores de los píxeles están en una escala de grises entre 0 y 255. Podemos normalizar los valores de los píxeles en el rango 0 y 1 dividiendo cada valor por el máximo valor, i.e., 255."]},{"cell_type":"code","metadata":{"id":"DByKdm21nDe3"},"source":["# normalize inputs from 0-255 to 0-1\n","X_train = X_train / 255\n","X_test = X_test / 255\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dUH7i1WCnDe3"},"source":["Finalmente, la variable de salida es un número entero de 0 a 9. Por tanto, usaremos One-Hot Encoding para transformar el vector de enteros de clase en una matriz binaria. \n","\n","Usaremos para ello la función de Keras `np_utils.to_categorical()`."]},{"cell_type":"code","metadata":{"id":"y-R_pVM3nDe3"},"source":["# one hot encode outputs\n","y_train = np_utils.to_categorical(y_train)\n","y_test = np_utils.to_categorical(y_test)\n","\n","num_classes = y_test.shape[1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qLJ0AsYTrAtN","executionInfo":{"status":"ok","timestamp":1620489030666,"user_tz":360,"elapsed":315,"user":{"displayName":"HENRRY WALDEMAR SONTAY CHAN","photoUrl":"","userId":"13180258940314725711"}},"outputId":"3f596645-882a-40f4-bbd7-6ef2bdd2e93a"},"source":["y_test.shape[1]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["10"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"HrFAZu3IQFsD"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"eWFbBy2pnDe3"},"source":["Vamos a definir nuestro modelo:\n","1. El número de entradas será el tamaño máximo de pixeles (784)\n","2. Tendrá una capa oculta con el mismo número de neuronas que entradas (784). \n","3. Se utiliza una función de activación ReLU en la capa oculta. \n","4. Se utiliza una función de activación Softmax en la capa de salida. \n","5. La función de pérdida será `categorical_crossentropy`. \n","6. Utilizaremos ADAM para aprender los pesos. \n","<img src=\"https://drive.google.com/uc?id=1swPTlvecinN9uuyl5LO0fKIsxniOkcwq\" width=\"671\" height=\"177\" />"]},{"cell_type":"code","metadata":{"id":"VEI33JAXnDe4"},"source":["# define baseline model\n","def baseline_model():\n","    # create model\n","    model = Sequential()\n","    model.add(Dense(num_pixeles, input_dim=num_pixeles, activation='relu'))\n","    model.add(Dense(num_classes, activation='softmax'))\n","    # Compile model\n","    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","    \n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bx1vazGpnDe4"},"source":["Entrenamos y evaluamos el modelo. \n","1. El modelo se ajusta a más de 10 épocas con actualizaciones cada 200 imágenes. \n","2. Los datos de test se utilizan como conjunto de datos de validación.\n","3. Se utiliza un valor `verbose` de 2. \n","4. Evaluamos en test e imprimimos las métricas."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9P89UYPnnDe4","executionInfo":{"status":"ok","timestamp":1620489813105,"user_tz":360,"elapsed":38026,"user":{"displayName":"HENRRY WALDEMAR SONTAY CHAN","photoUrl":"","userId":"13180258940314725711"}},"outputId":"fe7163ac-4896-438e-82a1-92e1f99ed2dc"},"source":["# build the model\n","model = baseline_model()\n","\n","# Fit the model\n","# verbose=0 will show you nothing (silent)\n","# verbose=1 will show you an animated progress bar like this:\n","# verbose=2 will just mention the number of epoch like this: Epoch 1/10\n","model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=2)\n","\n","# Final evaluation of the model\n","scores = model.evaluate(X_test, y_test, verbose=0)\n","print(\"Error del modelo de la linea base: %.2f%%\" % (100-scores[1]*100))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/10\n","300/300 - 4s - loss: 0.2757 - accuracy: 0.9227 - val_loss: 0.1365 - val_accuracy: 0.9618\n","Epoch 2/10\n","300/300 - 4s - loss: 0.1097 - accuracy: 0.9687 - val_loss: 0.0953 - val_accuracy: 0.9717\n","Epoch 3/10\n","300/300 - 4s - loss: 0.0713 - accuracy: 0.9794 - val_loss: 0.0775 - val_accuracy: 0.9771\n","Epoch 4/10\n","300/300 - 4s - loss: 0.0501 - accuracy: 0.9853 - val_loss: 0.0729 - val_accuracy: 0.9765\n","Epoch 5/10\n","300/300 - 4s - loss: 0.0370 - accuracy: 0.9896 - val_loss: 0.0695 - val_accuracy: 0.9787\n","Epoch 6/10\n","300/300 - 4s - loss: 0.0285 - accuracy: 0.9921 - val_loss: 0.0616 - val_accuracy: 0.9810\n","Epoch 7/10\n","300/300 - 4s - loss: 0.0210 - accuracy: 0.9939 - val_loss: 0.0624 - val_accuracy: 0.9807\n","Epoch 8/10\n","300/300 - 4s - loss: 0.0153 - accuracy: 0.9963 - val_loss: 0.0629 - val_accuracy: 0.9801\n","Epoch 9/10\n","300/300 - 4s - loss: 0.0101 - accuracy: 0.9980 - val_loss: 0.0560 - val_accuracy: 0.9829\n","Epoch 10/10\n","300/300 - 4s - loss: 0.0072 - accuracy: 0.9988 - val_loss: 0.0618 - val_accuracy: 0.9825\n","Error del modelo de la linea base: 1.75%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"H86uDB8YnDe5"},"source":["---\n","<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"ho7MyE08nDe5"},"source":["<a id=\"section4\"></a>\n","# <font color=\"#004D7F\" size=6>4. CNN para MNIST </font>"]},{"cell_type":"markdown","metadata":{"id":"bSdsGfWbnDe5"},"source":["Ahora que hemos visto cómo cargar el conjunto de datos MNIST y entrenar un modelo simple de perceptrón multicapa en él, es hora de desarrollar una red neuronal convolucional más sofisticada o un modelo CNN. \n","\n","Crearemos una CNN simple para MNIST que demuestra cómo utilizar todos los aspectos de una implementación de CNN moderna, incluidas las capas convolucionales, las capas de agrupación y las capas de dropout. \n","\n","El primer paso es importar las clases y funciones necesarias."]},{"cell_type":"code","metadata":{"id":"uQRwF3kYnDe5","executionInfo":{"status":"ok","timestamp":1651939325648,"user_tz":360,"elapsed":6570,"user":{"displayName":"HENRRY WALDEMAR SONTAY CHAN","userId":"13180258940314725711"}}},"source":["# Simple CNN for the MNIST Dataset\n","from keras.datasets import mnist\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import Dropout\n","from keras.layers import Flatten\n","from keras.layers.convolutional import Conv2D\n","from keras.layers.convolutional import MaxPooling2D\n","from keras.utils import np_utils\n","\n"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3Mdqwqd1nDe6"},"source":["En Keras, las capas utilizadas para convoluciones bidimensionales esperan valores de píxeles con las dimensiones `[muestras]-[ancho]-[alto]-[canales]`. \n","\n","En cuanto al canal en MNIST, ya que está dada en escala de grises, la dimensión de píxel se establece en 1."]},{"cell_type":"code","metadata":{"id":"jErc683MnDe6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651939376074,"user_tz":360,"elapsed":830,"user":{"displayName":"HENRRY WALDEMAR SONTAY CHAN","userId":"13180258940314725711"}},"outputId":"86661551-36f7-4c9e-d3ea-a6cb996a3102"},"source":["# load data\n","(X_train, y_train), (X_test, y_test) = mnist.load_data()\n","\n","# reshape to be [samples][width][height][channels]\n","X_train = X_train.reshape((X_train.shape[0], 28, 28, 1)).astype('float')\n","X_test = X_test.reshape((X_test.shape[0], 28, 28, 1)).astype('float')\n"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11493376/11490434 [==============================] - 0s 0us/step\n","11501568/11490434 [==============================] - 0s 0us/step\n"]}]},{"cell_type":"code","metadata":{"id":"zm8FXMvSu8tx"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"THgWiB53nDe6"},"source":["Normalizamos los valores de los píxeles en el rango 0 y 1 y realizar OHE en el target."]},{"cell_type":"code","metadata":{"id":"gJFJdOdVnDe6","executionInfo":{"status":"ok","timestamp":1651939421751,"user_tz":360,"elapsed":1157,"user":{"displayName":"HENRRY WALDEMAR SONTAY CHAN","userId":"13180258940314725711"}}},"source":["# normalize inputs from 0-255 to 0-1\n","X_train = X_train / 255\n","X_test = X_test / 255\n","\n","# one hot encode outputs\n","y_train = np_utils.to_categorical(y_train) \n","y_test = np_utils.to_categorical(y_test) \n","\n","num_classes = y_test.shape[1]"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8uLM_JoFnDe7"},"source":["A continuación, definimos nuestro modelo de red neuronal:\n","\n","1. La primera capa oculta es una capa convolucional llamada `Conv2D`. \n","    * Tiene 32 mapas de características, con un tamaño de $5 × 5$ y una función de activación ReLu.\n","2. Capa Pooling  `MaxPooling2D `. \n","    * Tamaño de pacht de $2 × 2$.\n","3. Capa de regularización `Dropout`. \n","4. Capa `Flatten` para conversión de la matriz 2D en un vector (1D).\n","5. Capa `Dense` con 128 neuronas y la función de activación ReLU.\n","6. Capa de salida con 10 neuronas para las 10 clases y una función de activación **Softmax**.\n","7. La compilación con ADAM, pérdida logarítmica como función de coste y Accuracy como métrica.\n","\n","<img src=\"https://drive.google.com/uc?id=1GkKR_mdfuDpH-CxJ0V2vZJ3m63CxM0-Z\" width=\"1487\" height=\"178\" />"]},{"cell_type":"code","metadata":{"id":"rYqlikeXnDe7","executionInfo":{"status":"ok","timestamp":1651940027062,"user_tz":360,"elapsed":164,"user":{"displayName":"HENRRY WALDEMAR SONTAY CHAN","userId":"13180258940314725711"}}},"source":["# define a simple CNN model\n","def baseline_model():\n","    # create model\n","    model = Sequential()\n","    model.add(Conv2D(32, (5,5), input_shape=(28,28,1), activation='relu'))\n","    model.add(MaxPooling2D())\n","    model.add(Dropout(0.2))\n","    model.add(Flatten())\n","    model.add(Dense(128, activation='relu'))\n","    model.add(Dense(num_classes, activation='softmax'))\n","\n","    # Compile model\n","    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","\n","    return model"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C8x0chjEnDe7"},"source":["Entrenamos con 10 épocas a un tamaño de batch de 200."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CQLwf75NnDe7","executionInfo":{"status":"ok","timestamp":1651940372605,"user_tz":360,"elapsed":325072,"user":{"displayName":"HENRRY WALDEMAR SONTAY CHAN","userId":"13180258940314725711"}},"outputId":"ab4e8a90-9053-485c-ccae-dc43c30d30ed"},"source":["# build the model\n","model = baseline_model()\n","\n","# Fit the model\n","model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200)\n","\n","# Final evaluation of the model\n","scores = model.evaluate(X_test, y_test, verbose=0)\n","print(\"Error del modelo de CNN linea base: %.2f%%\" % (100-scores[1]*100))"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","300/300 [==============================] - 33s 107ms/step - loss: 0.2540 - accuracy: 0.9263 - val_loss: 0.0816 - val_accuracy: 0.9754\n","Epoch 2/10\n","300/300 [==============================] - 30s 102ms/step - loss: 0.0757 - accuracy: 0.9777 - val_loss: 0.0475 - val_accuracy: 0.9861\n","Epoch 3/10\n","300/300 [==============================] - 31s 104ms/step - loss: 0.0510 - accuracy: 0.9842 - val_loss: 0.0390 - val_accuracy: 0.9875\n","Epoch 4/10\n","300/300 [==============================] - 31s 102ms/step - loss: 0.0404 - accuracy: 0.9875 - val_loss: 0.0355 - val_accuracy: 0.9895\n","Epoch 5/10\n","300/300 [==============================] - 30s 101ms/step - loss: 0.0315 - accuracy: 0.9904 - val_loss: 0.0353 - val_accuracy: 0.9877\n","Epoch 6/10\n","300/300 [==============================] - 31s 102ms/step - loss: 0.0269 - accuracy: 0.9915 - val_loss: 0.0367 - val_accuracy: 0.9888\n","Epoch 7/10\n","300/300 [==============================] - 30s 101ms/step - loss: 0.0221 - accuracy: 0.9928 - val_loss: 0.0332 - val_accuracy: 0.9900\n","Epoch 8/10\n","300/300 [==============================] - 30s 101ms/step - loss: 0.0177 - accuracy: 0.9946 - val_loss: 0.0340 - val_accuracy: 0.9879\n","Epoch 9/10\n","300/300 [==============================] - 30s 101ms/step - loss: 0.0151 - accuracy: 0.9951 - val_loss: 0.0313 - val_accuracy: 0.9900\n","Epoch 10/10\n","300/300 [==============================] - 30s 102ms/step - loss: 0.0138 - accuracy: 0.9954 - val_loss: 0.0374 - val_accuracy: 0.9885\n","Error del modelo de CNN linea base: 1.15%\n"]}]},{"cell_type":"markdown","metadata":{"id":"hJ31DLgQnDe8"},"source":["---\n","<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"hyc9tg3knDe8"},"source":["<a id=\"section5\"></a>\n","# <font color=\"#004D7F\" size=6>5. CNN más profunda para MNIST </font>"]},{"cell_type":"markdown","metadata":{"id":"RHxVhqc3nDe8"},"source":["Esta vez definimos una arquitectura con más capas de convolucionales, Max-pooling y capas completamente conectadas.\n","\n","1. Capa convolucional con 30 mapas de tamaño $5 × 5$.\n","2. Capa de Pooling con patch de $2 × 2$.\n","3. Capa convolucional con 15 mapas de tamaño $3 × 3$.\n","4. Capa de Pooling con patch de $2 × 2$.\n","5. Capa de Dropout del 20%.\n","6. Capa Flatten.\n","7. Capa completamente conectada con 128 neuronas y ReLu.\n","8. Capa completamente conectada con 50 neuronas y ReLu\n","9. Capa de salida cpm activación Softmax.\n","10. La compilación con ADAM, pérdida logarítmica como función de coste y Accuracy como métrica.\n","\n","<img src=\"https://drive.google.com/uc?id=1j5bxc-9useczn5A1B4gbzKEnJHAklmku\" width=\"2202\" height=\"179\" />"]},{"cell_type":"code","metadata":{"id":"ztTgFky2nDe8","outputId":"959acaee-2357-447a-d959-48d53553c413"},"source":["def larger_model():\n","    # create model\n","    ???\n","    # Compile model\n","    ???\n","    return model\n","\n","# build the model\n","???\n","\n","# Fit the model\n","???\n","\n","# Final evaluation of the model\n","???"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/10\n","300/300 [==============================] - 12s 41ms/step - loss: 0.3712 - accuracy: 0.8842 - val_loss: 0.0714 - val_accuracy: 0.9779\n","Epoch 2/10\n","300/300 [==============================] - 13s 45ms/step - loss: 0.0867 - accuracy: 0.9735 - val_loss: 0.0477 - val_accuracy: 0.9851\n","Epoch 3/10\n","300/300 [==============================] - 13s 43ms/step - loss: 0.0644 - accuracy: 0.9802 - val_loss: 0.0378 - val_accuracy: 0.9869\n","Epoch 4/10\n","300/300 [==============================] - 13s 45ms/step - loss: 0.0508 - accuracy: 0.9839 - val_loss: 0.0307 - val_accuracy: 0.9906\n","Epoch 5/10\n","300/300 [==============================] - 13s 44ms/step - loss: 0.0438 - accuracy: 0.9866 - val_loss: 0.0304 - val_accuracy: 0.9900\n","Epoch 6/10\n","300/300 [==============================] - 12s 41ms/step - loss: 0.0385 - accuracy: 0.9871 - val_loss: 0.0319 - val_accuracy: 0.9890\n","Epoch 7/10\n","300/300 [==============================] - 12s 41ms/step - loss: 0.0352 - accuracy: 0.9885 - val_loss: 0.0253 - val_accuracy: 0.9912\n","Epoch 8/10\n","300/300 [==============================] - 13s 42ms/step - loss: 0.0303 - accuracy: 0.9901 - val_loss: 0.0244 - val_accuracy: 0.9926\n","Epoch 9/10\n","300/300 [==============================] - 13s 42ms/step - loss: 0.0289 - accuracy: 0.9904 - val_loss: 0.0258 - val_accuracy: 0.9910\n","Epoch 10/10\n","300/300 [==============================] - 12s 41ms/step - loss: 0.0274 - accuracy: 0.9915 - val_loss: 0.0232 - val_accuracy: 0.9930\n","Large CNN Error: 0.70%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lk8eZ6eqnDe9"},"source":["<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n","\n","---\n","\n","<div style=\"text-align: right\"> <font size=6><i class=\"fa fa-coffee\" aria-hidden=\"true\" style=\"color:#004D7F\"></i> </font></div>"]}]}